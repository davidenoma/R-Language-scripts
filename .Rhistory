knitr::opts_chunk$set(fig.width = 5,fig.height = 5, size = "footnotesize", warning = FALSE, message = FALSE)
ls()
close(con)
bm = bodymap.eset
pdata = pData(bm)
edata = exprs(bm)
fdata = fData(bm)
table(pdata$gender)
table(pdata$gender, pdata$race)
summary(edata)
sum(pdata$age == ' ', na.rm = TRUE)
dim(edata)
is.na(edata)[1,]
#This sums all N/As and to check for NAs
sum(is.na(edata))
gene_na = rowSums(is.na(edata))
table(gene_na)
sampe_na = colSums(is.na(edata))
table(sample_na)
#Always check for missing values
c(dim(pdata), dim(edata), dim(fdata))
boxplot(edata[,1])
boxplot(log2(edata[,1]+1))
boxplot(log2(edata + 1),col=4,range = 0)
par(mfrom = c(1,2))
hist(log2(edata[,1]+1),col = 4)
hist(log2(edata[,2]+1),col = 4)
par(mfrom = c(1,1))
plot(density(log2(edata[,1]+1)),col = 3)
plot(density(log2(edata[,1]+1)),col = 4)
#The lines command is to allow overlay of the plots
lines(density(log2(edata[,2]+1)),col = 4)
qqplot(log2(edata[,1]+1),log2(edata[,2]+1))
abline(c(0,1))
##Ma plot
mm = log2(edata[,1] + 1) - log2(edata[,2] + 1)
aa = log2(edata[,1] + 1) + log2(edata[,2] + 1)
plot(aa, mm , col= 9)
edata = as.data.frame(edata)
#Removing row means less than 1 to eliminate the zero values
filt_edata  = filter(edata,rowMeans(edata) > 1)
dim(filt_edata)
boxplot(as.matrix(log2(filt_edata + 1)), col = 3)
#Compare datasets to annotation to ensure correctness
aeid = as.character(fdata[,1])
#to extract chromosome information
chr = AnnotationDbi::select(org.Hs.eg.db,keys = aeid, keytype = "ENSEMBL",columns = "CHR")
#Take non duplicated chromosomes
chr = chr[!duplicated(chr[,1]),]
dim(chr)
##Confirm that the annotation is still in the right order
all(chr[,1]== rownames(edata))
#Select the chromosome Y genes
edataY = dplyr::filter(edata,chr$CHR=="Y")
# We plot the sums of columns of samples bearing the Y chromosome against gender
boxplot(colSums(edataY) ~ pdata$gender)
points(colSums(edataY) ~ jitter(as.numeric(pdata$gender)),
col = as.numeric(pdata$gender),pch=19)
ematrix = as.matrix(edata) [rowMeans(edata) > 10000,]
heatmap(ematrix)
colramp = colorRampPalette(c(3,"white",2))(9)
heatmap(ematrix,col = colramp)
#To remove the automatic clustering
heatmap(ematrix,col = colramp, Rowv = NA, Colv = NA)
heatmap.2(ematrix,col = colramp, Rowv = NA, Colv = NA, dendogram = "none", scale = "row", trace = "none")
tropical = c('darkorange','dodgerblue','hotpink','limegreen','yellow')
palette(tropical)
par(pch=19)
library(devtools)
library(Biobase)
library(limma)
library(edge)
library(genefilter)
library(qvalue)
library(devtools)
library(Biobase)
library(limma)
library(edge)
library(genefilter)
install.packages('ggplot2')
library('ggplot2')
install.packages('ggplot2',dependencies = 'TRUE')
install.packages('ggplot2',dependencies = 'TRUE')
install.packages("ggplot2")
install.packages("ggplot2")
tropical = c('darkorange','dodgerblue','hotpink','limegreen','yellow')
palette(tropical)
par(pch=19)
library(devtools)
library(Biobase)
library(limma)
library(edge)
library(genefilter)
library(qvalue)
library(devtools)
library(devtools)
library(Biobase)
library(limma)
library(ggplot2)
ggplot(Orange, aes(x=age, y=circumference)) +
geom_point(color="#2980B9",size = 4) +
geom_smooth(method = lm, color = "#2C3E50")
fit = lm(circumference~ age, data = Orange)
summary(fit)
#Predict the circumference of a tree aged 1500
#From the statistics we cannot take the intercept because the value
#of X cannot be zero
age = 1500
cirm = 0.106 * age
cirm
#Calculating Confidence intervals in R
head(ToothGrowth)
##Confidence interval of mean
s = sd(ToothGrowth$len)
##Confidence interval of mean
s = sd(ToothGrowth$len)
standardError = s / sqrt(n)
zvalue = qnorm(0.975)
##Confidence interval of mean
n = len(ToothGrowth$len)
s = sd(ToothGrowth$len)
##Confidence interval of mean
n = length(ToothGrowth$len)
s = sd(ToothGrowth$len)
standardError = s / sqrt(n)
zvalue = qnorm(0.975)
zvalue
#Margin of error
moe = zvalue * standardError
xbar + c(-moe,moe)
xbar = mean(ToothGrowth$len)
xbar + c(-moe,moe)
tval = qt(0.975, df=n-1)
tval
zvalue
t.test(ToothGrowth$len) #95% ci for mean
t.test(ToothGrowth$len,conf.level = 0.9)
t.test(ToothGrowth$len,conf.level = 0.9)
t.test(ToothGrowth$len) #95% ci for mean
library(ggplot2)
#Statistical model is estimating parameter from a dataset.
fit = lm(circumference ~ age , data = Orange)
summary(fit)
ggplot(Orange, aes(x=age,y=circumference)) +
geom_point(color="#2990B9",size = 4) +
geom_smooth(method=lm, color = "#2C3E50")
new.data = data.frame(age=1500)
predict(fit,newdata = new.data, interval = "conf")
confint(fit)
summary(fit0)
#We can force intercept to be zero if we are sure that it is
#not significant
fit0 = lm(circumference ~ age + 0, data = Orange)
summary(fit0)
#RElationship between Anova and linear regression
fir = lm(Sepal.Length ~ Petal.Length, data=iris)
#RElationship between Anova and linear regression
fit1 = lm(Sepal.Length ~ Petal.Length, data=iris)
summary(fit1)
anova(fit1)
#Multiple Linear Regression
#One Response variable and two or more predictore
data(iris)
head(iris)
fit1 = lm(Sepal.Length ~ Sepal.Width + Petal.Length, data = iris)
summary(fit1)
fit2 = lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data = iris)
summary(fit2)
qplot(Sepal.Length, Petal.Length, data = iris)
fitlm(Petal.Length ~ Sepal.Length, data=iris)
summary(fitlm)
qplot(Sepal.Length, Petal.Length, data = iris)
fitlm(Petal.Length ~ Sepal.Length, data=iris)
summary(fitlm)
#Addition of categorical variables
#Accounting for interaction of predictor variables.
data(iris)
library(ggplot2)
qplot(Sepal.Length, Petal.Length, data = iris)
fitlm(Petal.Length ~ Sepal.Length, data=iris)
View(iris)
summary(fitlm)
fitlm = lm(Petal.Length ~ Sepal.Length, data=iris)
summary(fitlm)
summary(iris)
qplot(Sepal.length, Petal.Length, data=iris, color = Species)
qplot(Sepal.Length, Petal.Length, data=iris, color = Species)
x = lm(Petal.Length, Sepal.Length+Species, data = iris)
summary(x)
x = lm(Petal.Length ~ Sepal.Length+Species, data = iris)
summary(x)
#Is there a signiicant variation in petal length accross species
#
fit1 = lm(Petal.Length ~ Sepal.Length* Species, data = iris)
anova(fit1)
#anova show that the Species affects the Petal.Length at
#a significanct level i.e. predictor affecss the response
summary(iris$Species
)
summary(fit1)
#Check for conditions met for OLS regression
fit = lm(Sepal.Length ~ Petal.Length + Petal.Width, data=iris)
summary(fit)
par(mfrow=c(2,2))
plot(fit)
fir = lm(Sepal.Length ~ Petal.Length + Petal.Width, data=iris)
summary(fit)
plot(fit)
par(mfrow=c(2,2))
plot(fit)
install.packages('lmtest')
library('lmtest')
dwtest(fit)
install.packages('cat')
install.packages('car')
dwtest(fit)
library(car)
ncvTest(fit)
#Identify outliers that have too much influence of model
#influential datapoints
cutoff = 4 /((nrow(iris) - length(fit$coefficients)-2))
plot(fit, which = 4,cook.levels = cutoff)
par(mfrow=c(1,1))
plot(fit, which = 4,cook.levels = cutoff)
install.packages('mlbench')
library('mlbench')
library(help = mlbench)
data('Bostonhousing')
data(Bostonhousing)
data(BostonHousing)
str('BostonHousing')
str(BostonHousing)
summary(BostonHousing)
head(BostonHousing)
library(ggplot2)
library(car)
library(caret)
install.packages('carer')
install.packages('caret')
install.packages('corrplot')
library(caret)
library(corrplot)
#Dropping response variable Y
mat_a = subset(BostonHousing, select -c(medv))
#Dropping response variable Y
mat_a = subset(BostonHousing, select = -c(medv))
str(mat_a)
numeric <- mat_a[sapply(mat_a,is.numeric)]
#Calculating corrlation between two variables
numeric
print(descrCor)
#Calculating corrlation between two variables
descrCor <- cor(numeric)
print(descrCor)
corrplot(descrCor)
#Remove highly correlated variables
highlyCorrelated = findCorrelation(descrCor, cutoff = 0.7)
highlyCorCol
highlyCorCol = colnames(numeric)[highlyCorrelated]
highlyCorCol
#Remove highly correlated variables from the original dataset
#Create a new dataset.
data3 = BostonHousing[,-which(colnames(BostonHousing) %in% highlyCorCol)]
dim(data3)
names(data3)
corrplot(data3)
corrplot(cor(data3))
cor(data3)
summary(fit)
#Variance inflation factor(VIF) can handle co-linearity as well
fit = lm(medv~., data = BostonHousing )
summary(fit)
vif(fit)
##Check
df = cbind(BostonHousing$medv, data3)
df = as.data.frame(df)
fit2 = lm(medv~.,data=df)
vif(fit2)
data("BostonHousing")
library(mlbench
)
data("BostonHousing")
str(BostonHousing)
require(pls)
install.packages('pls')
require(pls)
set.seed(1000)
pcr_mode <- pcr(medv~.,data="Boston Housing", scale=TRUE,validation="CV" )
pcr_mode <- pcr(medv~.,data="Boston Housing", scale=TRUE,validation="CV" )
pcr_mode <- pcr(medv~.,data="BostonHousing", scale=TRUE,validation="CV" )
pcr_mode <- pcr(medv~.,data=BostonHousing,scale=TRUE,validation="CV" )
pcr_model <- pcr(medv~.,data=BostonHousing,scale=TRUE,validation="CV" )
summary(pcr_model)
validationplot(pcr_model)
validationplot(pcr_model,val.type = "R2")
predplot(pcr_model)
#Partial Least Square Regresssion
library(caret)
control <- trainControl('repeatedcv',index=myfolds, selectionFunction = "oneSE" )
myfolds <- createMultiFolds(BostonHousing$medv, k=5, times = 10)
control <- trainControl('repeatedcv',index=myfolds, selectionFunction = "oneSE" )
#Train the PLS model
mod1 <- train(medv ~ . , data = BostonHousing,
method="pls",
metric = "R2",
tuneLength = 20,
trControl = control,
preProc= c("zv", "center", "scale"))
plot(mod1)
install.packages('plsdepot')
##Try another dataset
libray(plsdepot)
##Try another dataset
library(plsdepot)
data(vehicles)
head(vehicles)
names(vehicles)
cars = vehicles [,c(1:13,15:16,13)]
]
cars = vehicles [,c(1:12,14:16,13)]
pls1 = plsreg1(cars[,1:15],cars[,16, drop=FALSE], comps = 5)
pls1
pls1$R2
data("BostonHousing")
require('tidyverse','broom','glmnet')
require(c('tidyverse','broom','glmnet'))
require(c('tidyverse','broom','glmnet'))
install.packages('tidyverse','broom','glmnet')
install.packages('tidyverse')
install.packages('broom')
install.packages('glmnet')
library('tidyverse','broom','glmnet')
library('tidyverse')
library('broom')
library('glmnet')
lamdas = 10 ^ seq(3, -2, by = .1)
lamdas = 10 ^ seq(3, -2, by = -.1)
lambdas
lambdas = 10 ^ seq(3, -2, by = -.1)
lambdas
y = BostonHousing$medv
x = BostonHousing %>%
select(crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat)
%>% data.matrix()
x = BostonHousing %>%
select(crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat)%>% data.matrix()
x = BostonHousing %>%select(crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat)%>% data.matrix()
fit <- glmnet(x,y,alpha = 0, lambda = lambdas)
fir
fit
#What is the optimal lambda value
cv_fit = cv.glmnet(x,y,alpha=0, lambda = lambdas)
plot(cv_fit)
optlambda <- cv_fit$lambda.min
optlambda
#predicting values and computing an R2 values for the data we trained on
y_predicted <- predict(fit, s = optlambda, newx= x )
#Sum of squares total and error
sst <- sum(y^2)
sse <- sum((y_predicted -y)^2)
rsq <- 1 - sse / sst
rsq
data("BostonHousing")
#Lasso finds the optimum lamda values
y = BostonHousing$medv
x = BostonHousing %>%select(crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat)%>% data.matrix()
install.packages(MASS)
install.packages('MASS')
install.packages("MASS")
install.packages("MASS")
fit =lm(Sepal.Length~., data=iris)
#To examine all models
step(lm(mpg~wt + drat + disp + qsec,data=mtcars),direction = "both")
#To examine all models
step(lm(mpg~wt + drat + disp + qsec,data=mtcars),direction = "both")
step(lm(mpg~wt + drat + disp + qsec,data=mtcars),direction = "backward")
#To examine all models
step(lm(mpg~wt + drat + disp + qsec,data=mtcars),direction = "both")
step(lm(mpg~wt + drat + disp + qsec,data=mtcars),direction = "backward")
step(null, scope = list(lower=null,upper = full), direction = "both
")
null = lm(SepalLength~1, data = iris)
#Mean value of Y predicts new Ys
full = lm(Sepal.Length~., data = iris) #include all Xs
step(null, scope = list(lower=null,upper = full), direction = "both")
step(null, scope = list(lower=null,upper = full), direction = "both")
null = lm(SepalLength~1, data = iris)
null = lm(Sepal.Length~1, data = iris)
#Mean value of Y predicts new Ys
full = lm(Sepal.Length~., data = iris) #include all Xs
step(null, scope = list(lower=null,upper = full), direction = "both")
install.packages('relaimpo')
library('relaimpo')
b
b
boot <- boot.relimp(fit, b = 1000, type = c("lmg","last","first","pratt"),
rank=TRUE,diff = TRUE , rela= TRUE)
#Bootstrap Measures of Relative importance
#Drawing randomly with replacement from a set of data points
boot <- boot.relimp(fit, b = 1000, type = c("lmg","last","first","pratt"),
rank=TRUE,diff = TRUE , rela= TRUE)
fit =lm(formula = Sepal.Length~ Petal.Length + Sepal.Width +Petal.Width, data=iris)
#Bootstrap Measures of Relative importance
#Drawing randomly with replacement from a set of data points
boot <- boot.relimp(fit, b = 1000, type = c("lmg","last","first","pratt"),
rank=TRUE,diff = TRUE , rela= TRUE)
booteval.relimp(boot)
plot(booteval.relimp(boot, sort=TRUE))
data(BostonHousing)
fit1 = lm(medv~.,data = BostonHousing)
summary(fit1)
fit2 = lm(medv~lstat, data=BostonHousing)
fit2 = lm(medv~lstat, data=BostonHousing)
summary(fit2)
install.packages(leaps)
install.packages('leaps')
library(leaps)
regFit = regsubsets(medv~.,data=BostonHousing)
summary(regFit)
reg.summary = summary(regFit)
reg.summary$rsq
plot(regFit, scale="adjr2", main="Adjusted R ^ 2")
#Evaluating the accuracy of a regression model-A machine learning approach
data("BostonHousing")
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
#Evaluating the accuracy of a regression model-A machine learning approach
data('BostonHousing')
#Evaluating the accuracy of a regression model-A machine learning approach
data('BostonHousing')
#Evaluating the accuracy of a regression model-A machine learning approach
data("BostonHousing")
#Evaluating the accuracy of a regression model-A machine learning approach
data(BostonHousing)
str(BostonHousing)
summary(BostonHousing)
library('caret')
set.seed(99)
Train = createDataPartition(BostonHousing$medv,p=0.75,list = FALSE)
#the ~. is to include all the predictor variables
fit1 = train(medv~., data=training, method="lm")
set.seed(99)
Train = createDataPartition(BostonHousing$medv,p=0.75,list = FALSE)
#75 25 SPLIT
training = BostonHousing[Train,]
testing = BostonHousing[-Train,]
#the ~. is to include all the predictor variables
fit1 = train(medv~., data=training, method="lm")
summary(fit1)
fit2 = train(medv~.,data=training,method = "lm", metric="RMSE")
print(fit2)
p1 = predict(fit2,newdata = testing)
p = as.data.frame(p1)
test = as.data.frame(testing$medv)
y = cbind(test,p)
colnames(y)=c("medv","Pred")
head(y)
cor.test(y$medv,y$Pred)
library(Metrics)
install.packages()
install.packages('Metrics')
library(Metrics)
rmse(y$medv, y$Pred)
train_control <- trainControl(method = "cv",number = 10)
fit3= train(medv~., data=BostonHousing,trControl=train_control, method="lm",metric="RMSE")
summary(fit3)
print(fit3)
train_control <- trainControl(method="cv",number = 10)
fit4 = train(medv~., data = training, trControl = train_control,method="lm")
print(fit4)
print(fit3)
colnames(y) = c("medv","Pred")
head(y)
cor.test(y$medv,y$Pred)
rmse(y$medv,y$Pred)
#LASSO REGRESSION FOR VARIABLE SELECTION
data(BostonHousing)
library(caret)
lasso <- train(medv~., BostonHousing,
method="lasso",
preProc = c('scale','center'),
trControl = train_control)
lasso
#Get Coefficient
predict.enet(lasso$finalModel,type='coefficients', s=lasso$bestTune$fraction)
library(relaimpo)
fit1 = lm(medv~.,data=BostonHousing)
calc.relimp(fit1, type=c("lmg"), rela = TRUE)
library(hier.part)
install.packages('hier.part')
x = BostonHousing[,1:12]
H = hier.part(BostonHousing$medv, x , fam="gaussian", gof = "Rsqu")
H = hier.part(BostonHousing$medv, x , fam="gaussian", gof = "Rsqu")
library(hier.part)
library(hier.part)
library(relaimpo)
install.packages('hier.part')
install.packages('hier.part')
library(hier.part)
library(hier.part)
install.packages('jier.part')
install.packages('hier.part')
install.packages('hier.part')
install.packages('hier.part',dependencies = TRUE)
