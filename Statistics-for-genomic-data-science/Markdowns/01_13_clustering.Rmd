---
title: Clustering
author: Jeff Leek
output:
  rmarkdown::html_document:
    toc: true
  vignette: >  
    %\VignetteIndexEntry{Clustering for genomics}
    %\VignetteEngine{knitr::rmarkdown}
    \usepackage[utf8]{inputenc}
---

```{r front, child="./../front.Rmd", echo=FALSE}
```


## Dependencies

This document depends on the following packages:

```{r load_hidden, echo=FALSE, results="hide", warning=FALSE}
suppressPackageStartupMessages({
  library(devtools)
  library(Biobase)
  library(dendextend)
})
```

```{r load}
  library(devtools)
  library(Biobase)
  library(dendextend)
```

To install these packages you can use the code (or if you are compiling the document, remove the `eval=FALSE` from the chunk.)

```{r install_packages, eval=FALSE}
install.packages(c("devtools","dendextend"))
source("http://www.bioconductor.org/biocLite.R")
biocLite(c("Biobase"))
```


## General principles

* How do we define close?
* How do we group things?
* How do we visualize the grouping? 
* How do we interpret the grouping? 


## Load some data

We will use this expression set to look at how we use plots and tables to check for different characteristics

```{r}
con =url("http://bowtie-bio.sourceforge.net/recount/ExpressionSets/bodymap_eset.RData")
load(file=con)
close(con)
bm = bodymap.eset
pdata=pData(bm)
edata=as.data.frame(exprs(bm))
fdata = fData(bm)
ls()
```

## Calculate distances between samples

* Most important step
  * Garbage in -> garbage out
* Distance or similarity
  * Continuous - euclidean distance
  * Continous - correlation similarity
  * Binary - manhattan distance
* Pick a distance/similarity that makes sense for your problem

First we log transform and remove lowly expressed genes, then calculate Euclidean distance

```{r}
edata = edata[rowMeans(edata) > 5000,]
edata = log2(edata + 1)

# By default calculates the distance between rows
dist1 = dist(t(edata))

## Look at distance matrix
colramp = colorRampPalette(c(3,"white",2))(9)
heatmap(as.matrix(dist1),col=colramp,Colv=NA,Rowv=NA)
```


## Now cluster the samples

Here we use the distance we previously calculated to perform a hierarchical clustering and plot the dendrogram:
```{r}
hclust1 = hclust(dist1)
plot(hclust1)
```

We can also force all of the leaves to terminate at the same spot

```{r}
plot(hclust1,hang=-1)
```

We can also color the dendrogram either into a fixed number of groups

```{r}
dend = as.dendrogram(hclust1)
dend = color_labels(hclust1,4,col=1:4)
plot(dend)
```

Or you can color them directly 

```{r}
labels_colors(dend) = c(rep(1,10),rep(2,9))
plot(dend)
```

This can get very fancy and we won't cover all the options here but for further ideas check out the [dendextend package](http://htmlpreview.github.io/?https://github.com/talgalili/dendextend/blob/master/inst/ignored/Introduction%20to%20dendextend.html).

## K-means clustering

Now we can perform k-means clustering. By default, the rows are clustered. You can either input the cluster means (often unknown) or the number of clusters (here I input 3 clusters)

```{r}
kmeans1 = kmeans(edata,centers=3)
names(kmeans1)
```

Now we can look at the cluster centers

```{r}
matplot(t(kmeans1$centers),col=1:3,type="l",lwd=3)
```

We can also look at how many belong to each cluster

```{r}
table(kmeans1$cluster)
```

And cluster the data together and plot

```{r}
heatmap(as.matrix(edata)[order(kmeans1$cluster),],col=colramp,Colv=NA,Rowv=NA)
```

Note that this is a random start so you get a different clustering if you run it again you get a different answer

```{r}
kmeans2 = kmeans(edata,centers=3)
table(kmeans1$cluster,kmeans2$cluster)
```


## Session information

Here is the session information 

```{r session_info}
devtools::session_info()
```

It is also useful to compile the time the document was processed. This document was processed on: `r Sys.Date()`.

